{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9/KTp+A4KHUazk3TxNMn7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esraa-abdelmaksoud/Shai-Training-Notebooks/blob/main/Twitter_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2Ew22CM24SQL"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import re\n",
        "from collections import Counter\n",
        "from itertools import chain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download some tweets\n",
        "nltk.download('twitter_samples')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XL_UJcV74lSH",
        "outputId": "d107f4f3-ae6e-48b5-8ac6-14ed10288c23"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all positive and negative tweets\n",
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ],
      "metadata": {
        "id": "ujPRFTAq4qpW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print tweets count\n",
        "print(f'positive count: {len(positive_tweets)}')\n",
        "print(f'negative count: {len(negative_tweets)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRIo8_WWbt--",
        "outputId": "76cf8fc0-8f61-4fe8-ad7f-eb43528e436d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive count: 5000\n",
            "negative count: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check data type\n",
        "print(type(positive_tweets))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwMj2kam46h_",
        "outputId": "9c78e680-be19-4f56-8a16-98bb5e3850a0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(r'\\w+')"
      ],
      "metadata": {
        "id": "ygrF6rIXmG_c"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load stop words\n",
        "nltk.download('stopwords')\n",
        "stopwords_list = stopwords.words('english') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgg_5okxwH8Z",
        "outputId": "34b70214-7cdf-4c6f-95c8-ef317dc358a5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load lemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlLCPEx3yNpH",
        "outputId": "6811f9c8-15aa-49ed-90c8-9f12bcb24caf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(positive_tweets)):\n",
        "  # Convert to lowercase\n",
        "  positive_tweets[i] = positive_tweets[i].lower()\n",
        "  negative_tweets[i] = negative_tweets[i].lower()\n",
        "  # Remove links\n",
        "  positive_tweets[i] = re.sub('(http)\\S+', '', positive_tweets[i])\n",
        "  negative_tweets[i] = re.sub('(http)\\S+', '', negative_tweets[i])\n",
        "  # Remove RT\n",
        "  positive_tweets[i] = re.sub('(RT)\\s(@)\\S+', '', positive_tweets[i])\n",
        "  negative_tweets[i] = re.sub('(RT)\\s(@)\\S+', '', negative_tweets[i])\n",
        "  # Remove mentions\n",
        "  positive_tweets[i] = re.sub('(@)\\S+', '', positive_tweets[i])\n",
        "  negative_tweets[i] = re.sub('(@)\\S+', '', negative_tweets[i])\n",
        "  # Remove hashtags\n",
        "  positive_tweets[i] = re.sub('(#)\\S+', '', positive_tweets[i])\n",
        "  negative_tweets[i] = re.sub('(#)\\S+', '', negative_tweets[i])\n",
        "  # Remove HTML tags\n",
        "  positive_tweets[i] = re.sub('[<](a href)*[/]*\\S+[>]', '', positive_tweets[i])\n",
        "  negative_tweets[i] = re.sub('[<](a href)*[/]*\\S+[>]', '', negative_tweets[i])\n",
        "  # Lemmatize text\n",
        "  positive_tweets[i] = lemmatizer.lemmatize(positive_tweets[i])\n",
        "  negative_tweets[i] = lemmatizer.lemmatize(negative_tweets[i])\n",
        "  # Tokenize tweets, remove emojis, punctuation\n",
        "  positive_tweets[i] = tokenizer.tokenize(positive_tweets[i])\n",
        "  negative_tweets[i] = tokenizer.tokenize(negative_tweets[i])\n",
        "  # Remove stop words\n",
        "  positive_tweets[i] = [word for word in positive_tweets[i] if word not in stopwords_list]\n",
        "  negative_tweets[i] = [word for word in negative_tweets[i] if word not in stopwords_list]\n"
      ],
      "metadata": {
        "id": "vYI8cymL5TyA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking a tweet after text preprocessing\n",
        "print(positive_tweets[20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTHHagR4x-2Y",
        "outputId": "9a947b44-a586-4c2e-d2d9-2241beb46a93"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['top', 'new', 'followers', 'community', 'week']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten lists\n",
        "pos_words = list(chain(*positive_tweets))\n",
        "neg_words = list(chain(*negative_tweets))"
      ],
      "metadata": {
        "id": "_de3zu_I5elq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting words\n",
        "pos_counts = Counter(pos_words,sorted=True)\n",
        "neg_counts = Counter(neg_words,sorted=True)"
      ],
      "metadata": {
        "id": "aPyQJbs-4iqy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting most common words\n",
        "mc_pos_keys = [pos_counts.most_common(20)[i][0] for i in range(20)] \n",
        "mc_neg_keys = [neg_counts.most_common(20)[i][0] for i in range(20)] "
      ],
      "metadata": {
        "id": "-8Ox8FT06P72"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mc_pos_keys)\n",
        "print(mc_neg_keys)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOL_VfXvSItU",
        "outputId": "5918a0c3-bc4f-45a1-8e14-e7d62517afce"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['thanks', 'follow', 'love', 'u', 'thank', 'good', 'like', 'day', 'happy', 'amp', 'great', 'hi', '3', 'get', 'see', 'back', 'know', 'lt', 'new', 'p']\n",
            "['please', 'miss', 'want', 'like', 'u', 'get', 'sorry', 'one', 'follow', 'time', 'much', 'go', 'really', 'love', 'know', 'im', 'still', 'sad', 'back', 'today']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lc_pos_keys = list(pos_counts)[-21:]\n",
        "lc_neg_keys = list(neg_counts)[-21:]"
      ],
      "metadata": {
        "id": "nWG_xiSrNYyJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lc_pos_keys)\n",
        "print(lc_neg_keys)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx2XFOOASb4h",
        "outputId": "75f7fb5e-92b2-4eef-f56e-404c99fc301b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fledged', 'workplace', 'venue', 'lagos', 'luxord', 'kingdom', 'potatos', 'hundreds', 'cited', 'academic', 'pokiri', '1nenokkadine', 'favs', 'heritage', 'wood', 'beleaf', 'peasant', 'ahahha', 'reminders', 'distant', 'adulthood']\n",
            "['konami', 'policy', 'pes', 'rantie', 'atm', 'perverse', 'bracelets', 'twins', 'bylfnnz', 'banned', 'press', 'duper', 'waaah', 'jaebum', 'ahmad', 'maslan', 'cooks', 'hull', 'supporter', 'expecting', 'misserable']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting least common words\n",
        "for i in range(20):\n",
        "  pos_counts.pop(mc_pos_keys[i])\n",
        "  neg_counts.pop(mc_neg_keys[i])\n",
        "  pos_counts.pop(lc_pos_keys[i])\n",
        "  neg_counts.pop(lc_neg_keys[i])\n",
        "  pos_words.remove(mc_pos_keys[i])\n",
        "  neg_words.remove(mc_neg_keys[i])\n",
        "  pos_words.remove(lc_pos_keys[i])\n",
        "  neg_words.remove(lc_neg_keys[i])"
      ],
      "metadata": {
        "id": "t834JGHcQPav"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pos_counts.most_common(20))\n",
        "print(neg_counts.most_common(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnhxcHjyRm0z",
        "outputId": "9ffccb84-e20a-4c01-979e-75ef51e696c4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('one', 127), ('hope', 123), ('today', 115), ('us', 115), ('time', 113), ('friday', 101), ('nice', 100), ('morning', 98), ('please', 96), ('let', 93), ('much', 89), ('would', 85), ('via', 85), ('go', 82), ('well', 81), ('really', 79), ('gt', 78), ('hey', 77), ('lot', 77), ('1', 75)]\n",
            "[('followed', 110), ('see', 108), ('amp', 101), ('good', 99), ('feel', 99), ('got', 99), ('day', 96), ('need', 95), ('wanna', 94), ('oh', 92), ('work', 91), ('wish', 88), ('going', 86), ('sleep', 82), ('thanks', 77), ('people', 76), ('would', 72), ('hope', 72), ('3', 72), ('could', 72)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pos_words[:20])\n",
        "print(neg_words[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lt0aRJL0TD9F",
        "outputId": "5f588679-c775-470f-9080-a39e196ed730"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['top', 'engaged', 'members', 'community', 'week', 'hey', 'james', 'odd', 'please', 'call', 'contact', 'centre', '02392441234', 'able', 'assist', 'many', 'listen', 'last', 'night', 'bleed']\n",
            "['hopeless', 'tmr', 'everything', 'kids', 'section', 'ikea', 'cute', 'shame', 'nearly', '19', '2', 'months', 'heart', 'sliding', 'waste', 'basket', 'hate', 'japanese', 'call', 'bani']\n"
          ]
        }
      ]
    }
  ]
}